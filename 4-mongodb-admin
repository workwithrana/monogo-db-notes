Backup
-------
mongodump
mongodump --dbpath D:\Rana2\custom_data -o D:\Rana2\custom_data\dump - server is not running (not present - deprecated)
mongodump --host localhost --port 30000
mongodump --collection collection --db test

Restore
-------
mongorestore dump
mongorestore --host localhost --port 30000 dump
mongorestore --dbpath D:\Rana2\custom_data D:\Rana2\custom_data\dump - server is not running - deprecated

Backup using copying the data dir - simply copy the dbpath

Monitoring
-----------
db.serverStatus()
db.runCommand( { serverStatus: 1, workingSet: 1, metrics: 0, locks: 0, tcmalloc : 0, wiredTiger : 0 } )

db.stats()
--------
db
Contains the name of the database.

collections
Contains a count of the number of collections in that database.

objects
Contains a count of the number of objects (i.e. documents) in the database across all collections.

avgObjSize
The average size of each document in bytes. This is the dataSize divided by the number of documents.

dataSize
The total size in bytes of the data held in this database including the padding factor. The scale argument affects this value. 
The dataSize will not decrease when documents shrink, but will decrease when you remove documents.

storageSize
The total amount of space in bytes allocated to collections in this database for document storage. 
The scale argument affects this value. The storageSize does not decrease as you remove or shrink documents.

numExtents
Contains a count of the number of extents in the database across all collections.

indexes
Contains a count of the total number of indexes across all collections in the database.

indexSize
The total size in bytes of all indexes created on this database. The scale arguments affects this value.

fileSize - for MMAPV1
The total size in bytes of the data files that hold the database. This value includes preallocated space and the padding factor. 
The value of fileSize only reflects the size of the data files for the database and not the namespace file.

dbStats.nsSizeMB - for MMAPV1
The total size of the namespace files (i.e. that end with .ns) for this database. You cannot change the size of the namespace file after creating a database, 
but you can change the default size for all new namespace files with the nsSize runtime option.

dataFileVersion - for MMAPV1
Document that contains information about the on-disk format of the data files for the database.

dbStats.dataFileVersion.major - for MMAPV1
The major version number for the on-disk format of the data files for the database.

dbStats.dataFileVersion.minor - for MMAPV1
The minor version number for the on-disk format of the data files for the database


collstats
---------

> db.users.stats()
{
	"ns" : "edureka.users",
	"count" : 6,
	"size" : 1724,
	"avgObjSize" : 287,
	"storageSize" : 36864,
	"capped" : false,
	"wiredTiger" : {
		"metadata" : {
			"formatVersion" : 1
		},
		"creationString" : "allocation_size=4KB,app_metadata=(formatVersion=1),block_allocation=best,block_compressor=snappy,cache_resident=0,checksum=on,colgroups=,collator=,columns=,dictionary=0,encryption=(keyid=,name=),exclusive=0,extractor=,format=btree,huffman_key=,huffman_value=,immutable=0,internal_item_max=0,internal_key_max=0,internal_key_truncate=,internal_page_max=4KB,key_format=q,key_gap=10,leaf_item_max=0,leaf_key_max=0,leaf_page_max=32KB,leaf_value_max=64MB,log=(enabled=),lsm=(auto_throttle=,bloom=,bloom_bit_count=16,bloom_config=,bloom_hash_count=8,bloom_oldest=0,chunk_count_limit=0,chunk_max=5GB,chunk_size=10MB,merge_max=15,merge_min=0),memory_page_max=10m,os_cache_dirty_max=0,os_cache_max=0,prefix_compression=0,prefix_compression_min=4,source=,split_deepen_min_child=0,split_deepen_per_child=0,split_pct=90,type=file,value_format=u",
		"type" : "file",
		"uri" : "statistics:table:collection-2--7045438039369430398",
		"LSM" : {
			"bloom filter false positives" : 0,
			"bloom filter hits" : 0,
			"bloom filter misses" : 0,
			"bloom filter pages evicted from cache" : 0,
			"bloom filter pages read into cache" : 0,
			"bloom filters in the LSM tree" : 0,
			"chunks in the LSM tree" : 0,
			"highest merge generation in the LSM tree" : 0,
			"queries that could have benefited from a Bloom filter that did not exist" : 0,
			"sleep for LSM checkpoint throttle" : 0,
			"sleep for LSM merge throttle" : 0,
			"total size of bloom filters" : 0
		},
		"block-manager" : {
			"allocations requiring file extension" : 0,
			"blocks allocated" : 0,
			"blocks freed" : 0,
			"checkpoint size" : 40960,
			"file allocation unit size" : 4096,
			"file bytes available for reuse" : 16384,
			"file magic number" : 120897,
			"file major version number" : 1,
			"file size in bytes" : 36864,
			"minor version number" : 0
		},
		"btree" : {
			"btree checkpoint generation" : 3,
			"column-store fixed-size leaf pages" : 0,
			"column-store internal pages" : 0,
			"column-store variable-size RLE encoded values" : 0,
			"column-store variable-size deleted values" : 0,
			"column-store variable-size leaf pages" : 0,
			"fixed-record size" : 0,
			"maximum internal page key size" : 368,
			"maximum internal page size" : 4096,
			"maximum leaf page key size" : 2867,
			"maximum leaf page size" : 32768,
			"maximum leaf page value size" : 67108864,
			"maximum tree depth" : 0,
			"number of key/value pairs" : 0,
			"overflow pages" : 0,
			"pages rewritten by compaction" : 0,
			"row-store internal pages" : 0,
			"row-store leaf pages" : 0
		},
		"cache" : {
			"bytes currently in the cache" : 2277,
			"bytes read into cache" : 1845,
			"bytes written from cache" : 0,
			"checkpoint blocked page eviction" : 0,
			"data source pages selected for eviction unable to be evicted" : 0,
			"hazard pointer blocked page eviction" : 0,
			"in-memory page passed criteria to be split" : 0,
			"in-memory page splits" : 0,
			"internal pages evicted" : 0,
			"internal pages split during eviction" : 0,
			"leaf pages split during eviction" : 0,
			"modified pages evicted" : 0,
			"overflow pages read into cache" : 0,
			"overflow values cached in memory" : 0,
			"page split during eviction deepened the tree" : 0,
			"page written requiring lookaside records" : 0,
			"pages read into cache" : 2,
			"pages read into cache requiring lookaside entries" : 0,
			"pages written from cache" : 0,
			"pages written requiring in-memory restoration" : 0,
			"unmodified pages evicted" : 0
		},
		"compression" : {
			"compressed pages read" : 0,
			"compressed pages written" : 0,
			"page written failed to compress" : 0,
			"page written was too small to compress" : 0,
			"raw compression call failed, additional data available" : 0,
			"raw compression call failed, no additional data available" : 0,
			"raw compression call succeeded" : 0
		},
		"cursor" : {
			"bulk-loaded cursor-insert calls" : 0,
			"create calls" : 1,
			"cursor-insert key and value bytes inserted" : 0,
			"cursor-remove key bytes removed" : 0,
			"cursor-update value bytes updated" : 0,
			"insert calls" : 0,
			"next calls" : 0,
			"prev calls" : 1,
			"remove calls" : 0,
			"reset calls" : 1,
			"restarted searches" : 0,
			"search calls" : 0,
			"search near calls" : 0,
			"truncate calls" : 0,
			"update calls" : 0
		},
		"reconciliation" : {
			"dictionary matches" : 0,
			"fast-path pages deleted" : 0,
			"internal page key bytes discarded using suffix compression" : 0,
			"internal page multi-block writes" : 0,
			"internal-page overflow keys" : 0,
			"leaf page key bytes discarded using prefix compression" : 0,
			"leaf page multi-block writes" : 0,
			"leaf-page overflow keys" : 0,
			"maximum blocks required for a page" : 0,
			"overflow values written" : 0,
			"page checksum matches" : 0,
			"page reconciliation calls" : 0,
			"page reconciliation calls for eviction" : 0,
			"pages deleted" : 0
		},
		"session" : {
			"object compaction" : 0,
			"open cursor count" : 1
		},
		"transaction" : {
			"update conflicts" : 0
		}
	},
	"nindexes" : 1,
	"totalIndexSize" : 16384,
	"indexSizes" : {
		"_id_" : 16384
	},
	"ok" : 1
}



mongostat
---------
mongostat.exe

mongotop
--------
mongotop.exe

Rest/Http

DB Profile
----------
profile level
0 - collects no data; profiler is off
1 - collects profiling data for slow operations only. By default slow operations are those slower than 100
milliseconds.
2 - collects profiling data for all database operations.

db.setProfilingLevel(2)
db.getProfilingStatus()
db.setProfilingLevel(1,200)

set profile level for entire mongod
------------------------------------
mongod --profile 1 --slowms 50



find slow queries
-------------------
db.system.profile.find().pretty()
db.system.profile.find( { op: { $ne : 'command' } } ).pretty()
db.system.profile.find().limit(10).sort( { ts : -1 } ).pretty()
db.system.profile.find( { ns : 'mydb.test' } ).pretty()
db.system.profile.find( { millis : { $gt : 5 } } ).pretty()

show profile


Export/Import
-------------
mongoexport.exe --collection class --db foo -o D:\Rana2\class.json
mongoexport.exe --collection class --db foo --type=csv -f _id,name,age,subject -o D:\Rana2\class.csv

mongoimport.exe --db test --collection class < D:\Rana2\class.json

db.categories.find( { path: /^,Books,/ } )

Explain Plan
---------------
> db.class.find().explain()
{
        "cursor" : "BasicCursor", -- whether it uses any index
        "isMultiKey" : false, -- true if uses a multi key index
        "n" : 30, -- number of documents that match the query selection criteria
        "nscannedObjects" : 30, -- total number of documents scanned.
        "nscanned" : 30, -- total number of index entries scanned (or documents for a collection scan)
        "nscannedObjectsAllPlans" : 30, -- number that reflects the total number of documents scanned for all query plans during the database operation.
        "nscannedAllPlans" : 30, -- number that reflects the total number of documents or index entries scanned for all query plans during the database operation
        "scanAndOrder" : false, -- true when the query cannot use the order of documents in the index for returning sorted results: 
								-- MongoDB must sort the documents after it receives the documents from a cursor.
        "indexOnly" : false, -- all the fields in the query are part of that index
        "nYields" : 0, -- reflects the number of times this query yielded the read lock to allow waiting writes to execute.
        "nChunkSkips" : 0, --  reflects the number of documents skipped because of active chunk migrations in a sharded system. 
							Typically this will be zero. A number greater than zero is ok, but indicates a little bit of inefficiency
        "millis" : 0, -- reflects the time in milliseconds to complete the query
        "server" : "BLRTCHAKR45477:27017",
        "filterSet" : false -- indicates whether MongoDB applied an index filter for the query
}
>

> db.class.find().explain('executionStats')

> db.class.find().explain('allPlansExecution')


Capped Collection
----------------
-- create a capped collection
db.createCollection( "log", { capped: true, size: 100000 } ) 
-- create a capped collection specifying max no of docs
db.createCollection("log", { capped : true, size : 5242880, max : 5000 } ) 

-- query a capped collection in the reverse insertion order
db.cappedCollection.find().sort( { $natural: -1 } )

--check if a collection is capped
db.collection.isCapped()

-- convert a collection to capped
db.runCommand({"convertToCapped": "log", size: 100000});


Expire Data on TTL
------------------

- create an TTL index on a BSON date-type field
- specify a non zero value in expireAfterSeconds field
- document will be deleted after the specified time
- mongodb runs a thread every 60 sec to delete the docs from the data file

db.log_events.ensureIndex( { "createdAt": 1 }, { expireAfterSeconds: 3600 } )

db.log_events.insert( {
	"createdAt": new Date(),
	"logEvent": 2,
	"logMessage": "Success!"
} )

-- to set expirtion at specific clock time
db.log_events.ensureIndex( { "expireAt": 1 }, { expireAfterSeconds: 0 } )

db.log_events.insert( {
	"expireAt": new Date('July 22, 2013 14:00:00'),
	"logEvent": 2,
	"logMessage": "Success!"
} )


Optimization Strategies
-----------------------

Evaluate Performance of Current Operations - db.currentOp(), DB profiling, explain()
Optimize Query Performance - index, projection, hint, $inc, 
Design Note 
- all the data modeling concepts and patterns,
- GriFS for large
- proper shard key
- single atomic transaction
- odd number of replica set

Log Rotation
------------
Start mongod with logappend
mongod -v --logpath /var/log/mongodb/server1.log --logappend 

Open mongo shell:
use admin
db.runCommand( {	 logRotate : 1 } )

Backup and restore with fsyncLock and fsyncUnlock
-------------------------------------------------
db.fsyncLock() -- lock all the writes temporarily to the disk
mongodump -- generate the bson dump
db.fsyncUnlock() -- unlock all the writes

Recover from unexpected shutdown
--------------------------------
Journaling Enabled
Replica Set

If above two are not there:
1. Start mongod using --repair to read the existing data files.
mongod --dbpath /data/db --repair --repairpath /data/db0
When this completes, the new repaired data files will be in the /data/db0 directory.
2. Start mongod using the following invocation to point the dbPath at /data/db0:
mongod --dbpath /data/db0
